{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "f69c9cfc-52a3-4a29-a008-71dc5397e516"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7aa3c2d54da0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8_pYr7PZi2Sn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "5e3d4a39-dc7f-4dd9-f54f-5e23377b38e7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "model is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3040929780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Keep it simple: CartPole isn't worth deep architectures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model is not defined\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: model is not defined"
          ]
        }
      ],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = None\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)  # Do NOT apply softmax here\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "model = PolicyNetwork(state_dim[0], n_actions)\n"
      ],
      "metadata": {
        "id": "l-D8WMUMqu_H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2hwdClocgO",
        "outputId": "dd6a3bed-aeb2-4944-83c1-996c5c9b48f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    probs = None\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    states_tensor = torch.from_numpy(states).float()\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation\n",
        "        logits = model(states_tensor)          # [batch, n_actions]\n",
        "        probs = F.softmax(logits, dim=1).numpy()  # convert to numpy\n",
        "\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "F5v2dheVrEj-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my 2\n",
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states using a model.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :return: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    states_tensor = torch.from_numpy(states).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(states_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "MDpMn_hC6o-k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states, model)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted probabilities:\\n\", test_probas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df0DVIUAyGEp",
        "outputId": "65fa12be-0138-4187-b4b2-b50626c76c83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilities:\n",
            " [[0.47153106 0.5284689 ]\n",
            " [0.46960354 0.53039646]\n",
            " [0.46693683 0.53306323]\n",
            " [0.4673471  0.532653  ]\n",
            " [0.47119883 0.5288012 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]), model)[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5sdENWJAi2Sz"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    cumulative_rewards = None\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my code\n",
        "def get_cumulative_rewards(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Compute discounted cumulative rewards G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "    :param rewards: list or array of immediate rewards\n",
        "    :param gamma: discount factor\n",
        "    :return: numpy array of cumulative rewards\n",
        "    \"\"\"\n",
        "    cumulative_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
        "    G = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G\n",
        "        cumulative_rewards[t] = G\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "    return cumulative_rewards\n"
      ],
      "metadata": {
        "id": "b0SrVum4r5_y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "metadata": {
        "id": "-eR7JUT9J3Gb",
        "outputId": "770cfa48-0928-40d9-d4df-2b1e46601ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9d4tXkpocga"
      },
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "urVfMFLaocgb"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = None\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = None\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = None # [batch,]\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = None  # a number\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = None\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    loss = None\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my code\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the REINFORCE loss with optional entropy regularization.\n",
        "\n",
        "    :param logits: [batch, n_actions] output of the policy network\n",
        "    :param actions: list or array of taken actions [batch,]\n",
        "    :param rewards: list or array of rewards [batch,]\n",
        "    :param gamma: discount factor\n",
        "    :param entropy_coef: coefficient for entropy regularization\n",
        "    :return: loss tensor\n",
        "    \"\"\"\n",
        "    # Convert actions and compute cumulative discounted rewards\n",
        "    actions = torch.tensor(actions, dtype=torch.int64)\n",
        "    cumulative_returns = torch.tensor(get_cumulative_rewards(rewards, gamma), dtype=torch.float32)\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (cumulative_returns.std() + 1e-8)\n",
        "\n",
        "    # Compute action probabilities and log-probabilities\n",
        "    probs = F.softmax(logits, dim=1)        # [batch, n_actions]\n",
        "    log_probs = F.log_softmax(logits, dim=1)  # [batch, n_actions]\n",
        "\n",
        "    # Select log-probs for the actions actually taken\n",
        "    log_probs_for_actions = log_probs[range(len(actions)), actions]  # [batch,]\n",
        "\n",
        "    # REINFORCE objective (maximize expected return)\n",
        "    J_hat = log_probs_for_actions * cumulative_returns  # [batch,]\n",
        "\n",
        "    # Entropy regularization to encourage exploration\n",
        "    entropy = -torch.sum(probs * log_probs, dim=1)  # [batch,]\n",
        "\n",
        "    # Final loss (note: we minimize -J_hat)\n",
        "    loss = -torch.mean(J_hat + entropy_coef * entropy)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "Qc3FnJNVsv5q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my code 2\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute REINFORCE loss given logits (policy network output), actions and rewards.\n",
        "    :param logits: torch tensor [batch, n_actions]\n",
        "    :param actions: list/array of taken actions [batch,]\n",
        "    :param rewards: list/array of rewards [batch,]\n",
        "    :param gamma: discount factor\n",
        "    :param entropy_coef: entropy regularization coefficient\n",
        "    :return: torch scalar loss\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int64)\n",
        "\n",
        "    # Cumulative rewards and normalize\n",
        "    cumulative_returns = torch.tensor(get_cumulative_rewards(rewards, gamma), dtype=torch.float32)\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (cumulative_returns.std() + 1e-8)\n",
        "\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "    log_probs_for_actions = log_probs[range(len(actions)), actions]  # [batch,]\n",
        "    J_hat = log_probs_for_actions * cumulative_returns\n",
        "\n",
        "    entropy = -torch.sum(probs * log_probs, dim=1)  # [batch,]\n",
        "    loss = -torch.mean(J_hat + entropy_coef * entropy)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "-8lZWhTWzMTT"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my code 3\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    # cumulative discounted rewards\n",
        "    cumulative_returns = torch.tensor(\n",
        "        get_cumulative_rewards(rewards, gamma), dtype=torch.float32\n",
        "    )\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (\n",
        "        cumulative_returns.std() + 1e-8\n",
        "    )\n",
        "\n",
        "    # probabilities and log-probabilities\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    log_probs = torch.log_softmax(logits, dim=1)\n",
        "\n",
        "    # convert actions to one-hot\n",
        "    actions = torch.tensor(actions, dtype=torch.int64)\n",
        "    one_hot_actions = to_one_hot(actions, logits.shape[1])  # [batch, n_actions]\n",
        "\n",
        "    # log-probs only for chosen actions\n",
        "    log_probs_for_actions = torch.sum(log_probs * one_hot_actions, dim=1)\n",
        "\n",
        "    # REINFORCE objective\n",
        "    J_hat = log_probs_for_actions * cumulative_returns\n",
        "\n",
        "    # entropy bonus\n",
        "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
        "\n",
        "    # final loss (maximize J_hat â†’ minimize -J_hat)\n",
        "    loss = -torch.mean(J_hat + entropy_coef * entropy)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "n4F3jRG1jXjh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my code 4\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    # cumulative discounted rewards\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = torch.log_softmax(logits, dim=1)\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    actions = sourceTensor.detach().clone()\n",
        "    one_hot_actions = to_one_hot(actions, logits.shape[1])  # [batch, n_actions]\n",
        "    log_probs_for_actions = torch.sum(log_probs * one_hot_actions, dim=1)\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "\n",
        "    J_hat = log_probs_for_actions * cumulative_returns\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = torch.sum(probs * log_probs, dim=1)\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "\n",
        "    loss = -torch.mean(J_hat - entropy_coef * entropy)\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JD--vSJJN1aw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckHj5sXBi2TE",
        "outputId": "883120fb-6988-4cdc-fefa-e160b08a09fc",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-905914556.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(actions, dtype=torch.int32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:996.530\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "g1zjhdZVocge"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "n_actions = env_for_video.action_space.n\n",
        "\n",
        "episode_index = 0\n",
        "step_starting_index = 0\n",
        "\n",
        "obs, info = env_for_video.reset()\n",
        "\n",
        "for step_index in range(800):\n",
        "    probs = predict_probs(np.array([obs]), model)[0]\n",
        "    action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done or step_index == 799:\n",
        "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "        frames = env_for_video.render()\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        save_video(\n",
        "            frames, \"videos\",\n",
        "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "            step_starting_index=step_starting_index,\n",
        "            episode_index=episode_index,\n",
        "        )\n",
        "        episode_index += 1\n",
        "        step_starting_index = step_index + 1\n",
        "        obs, info = env_for_video.reset()\n",
        "\n",
        "env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiI_Uhm2ocgf"
      },
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELl0c6tSocgf"
      },
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRb6eKGWocgg"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8jNcPjeocgh"
      },
      "outputs": [],
      "source": [
        "# Your brave and victorious code here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}