{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "7b399ea0-cea0-421b-b6b4-5185cb7d6843"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7c4e926f9580>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8_pYr7PZi2Sn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "343e429e-348d-491b-b025-ac678cfb20ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "model is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3040929780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Keep it simple: CartPole isn't worth deep architectures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model is not defined\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: model is not defined"
          ]
        }
      ],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = None\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)  # Do NOT apply softmax here\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model\n",
        "model = PolicyNetwork(state_dim[0], n_actions)\n"
      ],
      "metadata": {
        "id": "l-D8WMUMqu_H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2hwdClocgO",
        "outputId": "20fe2641-98b2-458f-ee3f-5215e25085ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    probs = None\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    states_tensor = torch.from_numpy(states).float()\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation\n",
        "        logits = model(states_tensor)          # [batch, n_actions]\n",
        "        probs = F.softmax(logits, dim=1).numpy()  # convert to numpy\n",
        "\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "F5v2dheVrEj-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my 2\n",
        "def predict_probs(states, model):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states using a model.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :return: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    states_tensor = torch.from_numpy(states).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(states_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).numpy()\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "MDpMn_hC6o-k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states, model)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted probabilities:\\n\", test_probas)\n"
      ],
      "metadata": {
        "id": "df0DVIUAyGEp",
        "outputId": "65fa12be-0138-4187-b4b2-b50626c76c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilities:\n",
            " [[0.47153106 0.5284689 ]\n",
            " [0.46960354 0.53039646]\n",
            " [0.46693683 0.53306323]\n",
            " [0.4673471  0.532653  ]\n",
            " [0.47119883 0.5288012 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]), model)[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5sdENWJAi2Sz"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    cumulative_rewards = None\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my code\n",
        "def get_cumulative_rewards(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Compute discounted cumulative rewards G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "    :param rewards: list or array of immediate rewards\n",
        "    :param gamma: discount factor\n",
        "    :return: numpy array of cumulative rewards\n",
        "    \"\"\"\n",
        "    cumulative_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
        "    G = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G\n",
        "        cumulative_rewards[t] = G\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "    return cumulative_rewards\n"
      ],
      "metadata": {
        "id": "b0SrVum4r5_y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DX39wcUi2S3",
        "outputId": "1b4b8acb-7481-4972-b6b8-327610ed14d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "def get_cumulative_rewards(rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Compute discounted cumulative rewards G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "    :param rewards: list or array of immediate rewards\n",
        "    :param gamma: discount factor\n",
        "    :return: numpy array of cumulative rewards\n",
        "    \"\"\"\n",
        "    cumulative_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
        "    G = 0.0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G\n",
        "        cumulative_rewards[t] = G\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "    return cumulative_rewards\n",
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9d4tXkpocga"
      },
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "urVfMFLaocgb"
      },
      "outputs": [],
      "source": [
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = None\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = None\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = None # [batch,]\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = None  # a number\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "\n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = None\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    loss = None\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#my code\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the REINFORCE loss with optional entropy regularization.\n",
        "\n",
        "    :param logits: [batch, n_actions] output of the policy network\n",
        "    :param actions: list or array of taken actions [batch,]\n",
        "    :param rewards: list or array of rewards [batch,]\n",
        "    :param gamma: discount factor\n",
        "    :param entropy_coef: coefficient for entropy regularization\n",
        "    :return: loss tensor\n",
        "    \"\"\"\n",
        "    # Convert actions and compute cumulative discounted rewards\n",
        "    actions = torch.tensor(actions, dtype=torch.int64)\n",
        "    cumulative_returns = torch.tensor(get_cumulative_rewards(rewards, gamma), dtype=torch.float32)\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (cumulative_returns.std() + 1e-8)\n",
        "\n",
        "    # Compute action probabilities and log-probabilities\n",
        "    probs = F.softmax(logits, dim=1)        # [batch, n_actions]\n",
        "    log_probs = F.log_softmax(logits, dim=1)  # [batch, n_actions]\n",
        "\n",
        "    # Select log-probs for the actions actually taken\n",
        "    log_probs_for_actions = log_probs[range(len(actions)), actions]  # [batch,]\n",
        "\n",
        "    # REINFORCE objective (maximize expected return)\n",
        "    J_hat = log_probs_for_actions * cumulative_returns  # [batch,]\n",
        "\n",
        "    # Entropy regularization to encourage exploration\n",
        "    entropy = -torch.sum(probs * log_probs, dim=1)  # [batch,]\n",
        "\n",
        "    # Final loss (note: we minimize -J_hat)\n",
        "    loss = -torch.mean(J_hat + entropy_coef * entropy)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "Qc3FnJNVsv5q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my code 2\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute REINFORCE loss given logits (policy network output), actions and rewards.\n",
        "    :param logits: torch tensor [batch, n_actions]\n",
        "    :param actions: list/array of taken actions [batch,]\n",
        "    :param rewards: list/array of rewards [batch,]\n",
        "    :param gamma: discount factor\n",
        "    :param entropy_coef: entropy regularization coefficient\n",
        "    :return: torch scalar loss\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int64)\n",
        "\n",
        "    # Cumulative rewards and normalize\n",
        "    cumulative_returns = torch.tensor(get_cumulative_rewards(rewards, gamma), dtype=torch.float32)\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (cumulative_returns.std() + 1e-8)\n",
        "\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "    log_probs_for_actions = log_probs[range(len(actions)), actions]  # [batch,]\n",
        "    J_hat = log_probs_for_actions * cumulative_returns\n",
        "\n",
        "    entropy = -torch.sum(probs * log_probs, dim=1)  # [batch,]\n",
        "    loss = -torch.mean(J_hat + entropy_coef * entropy)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "-8lZWhTWzMTT"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my code 3\n",
        "\n",
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    # cumulative discounted rewards\n",
        "    cumulative_returns = torch.tensor(\n",
        "        get_cumulative_rewards(rewards, gamma), dtype=torch.float32\n",
        "    )\n",
        "    cumulative_returns = (cumulative_returns - cumulative_returns.mean()) / (\n",
        "        cumulative_returns.std() + 1e-8\n",
        "    )\n",
        "\n",
        "    # probabilities and log-probabilities\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    log_probs = torch.log_softmax(logits, dim=1)\n",
        "\n",
        "    # convert actions to one-hot\n",
        "    actions = torch.tensor(actions, dtype=torch.int64)\n",
        "    one_hot_actions = to_one_hot(actions, logits.shape[1])  # [batch, n_actions]\n",
        "\n",
        "    # log-probs only for chosen actions\n",
        "    log_probs_for_actions = torch.sum(log_probs * one_hot_actions, dim=1)\n",
        "\n",
        "    # REINFORCE objective\n",
        "    J_hat = log_probs_for_actions * cumulative_returns\n",
        "\n",
        "    # entropy bonus\n",
        "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
        "\n",
        "    # final loss (maximize J_hat â†’ minimize -J_hat)\n",
        "    loss = -torch.mean(J_hat + entropy_coef * entropy)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "n4F3jRG1jXjh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "ckHj5sXBi2TE",
        "outputId": "bbe2455e-e9ed-4352-a6fe-02a9ec7bee67",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2359954175.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "get_loss() got an unexpected keyword argument 'n_actions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-841314614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_on_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2359954175.py\u001b[0m in \u001b[0;36mtrain_on_session\u001b[0;34m(states, actions, rewards, gamma, entropy_coef)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# cast everything into torch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentropy_coef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Gradient descent step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_loss() got an unexpected keyword argument 'n_actions'"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "g1zjhdZVocge"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.save_video import save_video\n",
        "\n",
        "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "n_actions = env_for_video.action_space.n\n",
        "\n",
        "episode_index = 0\n",
        "step_starting_index = 0\n",
        "\n",
        "obs, info = env_for_video.reset()\n",
        "\n",
        "for step_index in range(800):\n",
        "    probs = predict_probs(np.array([obs]), model)[0]\n",
        "    action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if done or step_index == 799:\n",
        "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "        frames = env_for_video.render()\n",
        "        os.makedirs(\"videos\", exist_ok=True)\n",
        "        save_video(\n",
        "            frames, \"videos\",\n",
        "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "            step_starting_index=step_starting_index,\n",
        "            episode_index=episode_index,\n",
        "        )\n",
        "        episode_index += 1\n",
        "        step_starting_index = step_index + 1\n",
        "        obs, info = env_for_video.reset()\n",
        "\n",
        "env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiI_Uhm2ocgf"
      },
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELl0c6tSocgf"
      },
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRb6eKGWocgg"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8jNcPjeocgh"
      },
      "outputs": [],
      "source": [
        "# Your brave and victorious code here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}